{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Songs Scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting yt-dlp\n",
      "  Downloading yt_dlp-2025.2.19-py3-none-any.whl.metadata (171 kB)\n",
      "Downloading yt_dlp-2025.2.19-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.5/3.2 MB 1.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.8/3.2 MB 1.7 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 1.0/3.2 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 1.6/3.2 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.1/3.2 MB 1.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.6/3.2 MB 1.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.1/3.2 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.2/3.2 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: yt-dlp\n",
      "Successfully installed yt-dlp-2025.2.19\n"
     ]
    }
   ],
   "source": [
    "!pip install yt-dlp\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Videos Metadata from Youtube (Title , Channel , Channel's Subscriber Count , Views , Likes , URL )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem with Scraper :**\n",
    "\n",
    "I have added proper random sleep time between 5sec to 10sec but still you can get Error : 429 (Too Many Requests) from server and server wil block your IP for sending so many requests.So try to run this code only once or twice. If you have a static IP than that is a serious issue you cannot access youtube becuase your IP is blocked by youtube servers otherwise if you have Dynamic IP then dont worry you will be able to access youtube as soon as you IP changes\n",
    "\n",
    "**Solution :**\n",
    "1. Use VPN \n",
    "2. Use kaggle or  colab to run this code (if kaggle/colab ip got blocked further scrapping can't be done but your own local IP is safe)\n",
    "3. Increase sleep time range i.e between 20sec to 30sec in  search_youtube_videos() function (This will take longer time to scrape)\n",
    "\n",
    "I have automated everything here just run the code and go back to your lazy life :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt_dlp\n",
    "import csv\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def parse_views(view_str):\n",
    "    \"\"\"Convert view/like counts from string '1,234,567' to integer 1234567\"\"\"\n",
    "    if isinstance(view_str, (int, float)):\n",
    "        return int(view_str)\n",
    "    return int(view_str.replace(',', '')) if view_str else 0\n",
    "\n",
    "def get_best_match_ratio(small: str, large: str) -> float:\n",
    "    \"\"\"Find the best similarity ratio by sliding over the larger string.\"\"\"\n",
    "    best_ratio = 0.0\n",
    "    len_small = len(small)\n",
    "    \n",
    "    for i in range(len(large) - len_small + 1):\n",
    "        segment = large[i : i + len_small]  # Extract substring of the same length as `small`\n",
    "        ratio = SequenceMatcher(None, small, segment).ratio()\n",
    "        best_ratio = max(best_ratio, ratio)\n",
    "\n",
    "    return best_ratio\n",
    "\n",
    "def title_similarity(str1,str2,threshold=0.80) :\n",
    "    \"\"\"Check if smaller string matches at least `threshold` percent within the larger string.\"\"\"\n",
    "    small, large = sorted([str1, str2], key=len)\n",
    "    \n",
    "    match_ratio = get_best_match_ratio(small, large)\n",
    "    \n",
    "    return match_ratio >= threshold\n",
    "\n",
    "def get_video_engagement(video):\n",
    "    \"\"\"Extract and convert engagement metrics with fallback values\"\"\"\n",
    "    return {\n",
    "        'views': parse_views(video.get('view_count', 0)),\n",
    "        'subscribers': parse_views(video.get('channel_follower_count', 0)),\n",
    "        'likes': parse_views(video.get('like_count', 0)),\n",
    "        'duration': video.get('duration', 0),\n",
    "        'title': video.get('title', ''),\n",
    "        'url': video.get('webpage_url', ''),\n",
    "        'channel': video.get('uploader', '')\n",
    "    }\n",
    "\n",
    "def is_better_candidate(current, new_candidate):\n",
    "    \"\"\"Compare two videos to determine which has better engagement\"\"\"\n",
    "    return (\n",
    "        (new_candidate['views'] > current['views']) or\n",
    "        (new_candidate['views'] == current['views'] and \n",
    "         new_candidate['subscribers'] > current['subscribers']) or\n",
    "        (new_candidate['views'] == current['views'] and\n",
    "         new_candidate['subscribers'] == current['subscribers'] and\n",
    "         new_candidate['likes'] > current['likes'])\n",
    "    )\n",
    "\n",
    "def filter_videos(videos, max_duration, min_views, min_subs, exclude_keywords):\n",
    "    \"\"\"Process videos with all filters and duplicate handling\"\"\"\n",
    "    filtered = {}\n",
    "    exclude_set = {kw.lower() for kw in (exclude_keywords or [])}\n",
    "\n",
    "    for video in videos:\n",
    "        if not video:\n",
    "            continue\n",
    "\n",
    "        eng = get_video_engagement(video)\n",
    "        \n",
    "        # Basic filters\n",
    "        if (eng['duration'] > max_duration or\n",
    "            eng['views'] < min_views or\n",
    "            eng['subscribers'] < min_subs or\n",
    "            any(kw in eng['title'].lower() for kw in exclude_set)):\n",
    "            continue\n",
    "\n",
    "        # Duplicate handling\n",
    "        found_duplicate = False\n",
    "        for key in list(filtered.keys()):\n",
    "            if title_similarity(key, eng['title']):\n",
    "                found_duplicate = True\n",
    "                if is_better_candidate(filtered[key], eng):\n",
    "                    del filtered[key]\n",
    "                    filtered[eng['title']] = eng\n",
    "                break\n",
    "        \n",
    "        if not found_duplicate:\n",
    "            filtered[eng['title']] = eng\n",
    "\n",
    "    return sorted(filtered.values(), \n",
    "                 key=lambda x: (-x['views'], -x['subscribers'], -x['likes']))\n",
    "\n",
    "def search_youtube_videos(query,csv_path,max_results=10, max_duration=1200, \n",
    "                         min_views=100000, min_subs=100000, exclude_keywords=None):\n",
    "    search_query = f\"ytsearch{max_results}:{query}\"  # Search for videos\n",
    "\n",
    "    ydl_opts = {\n",
    "        \"quiet\": True,\n",
    "        \"default_search\": \"ytsearch\",   # Search YouTube\n",
    "        \"extract_flat\": False,          # Get full metadata\n",
    "        \"sleep_interval\": 5,            # Wait 5 seconds between downloads\n",
    "        \"max_sleep_interval\": 10        # Randomize wait up to 10 seconds\n",
    "    }\n",
    "\n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        info = ydl.extract_info(search_query, download=False)  \n",
    "        videos = info.get('entries', []) if info else []\n",
    "\n",
    "    filtered = filter_videos(videos, max_duration, min_views, \n",
    "                            min_subs, exclude_keywords)\n",
    "\n",
    "    # Prepare CSV data\n",
    "    csv_data = [{\n",
    "        'Title': v['title'],\n",
    "        'Channel': v['channel'],\n",
    "        'Subscribers': f\"{v['subscribers']:,}\",\n",
    "        'Views': f\"{v['views']:,}\",\n",
    "        'Likes': f\"{v['likes']:,}\",\n",
    "        'Duration (seconds)': v['duration'],\n",
    "        'URL': v['url']\n",
    "    } for v in filtered]\n",
    "\n",
    "    # Write to CSV\n",
    "    csv_file = csv_path + \".csv\"\n",
    "    with open(csv_file, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=csv_data[0].keys() if csv_data else [])\n",
    "        writer.writeheader()\n",
    "        writer.writerows(csv_data)\n",
    "\n",
    "    return filtered, csv_file\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  Search_Queries = [\"Nayyara Noor Songs\",\"Tina Sani Songs\",\"Lata Mangeshkar Solo Songs\",\"Muhammad Rafi Solo Songs\",\"Asha Bhosle Solo Songs\"]\n",
    "  CSV_Paths = [\"Nayyara_Noor_Songs\",\"Tina_Sani_Songs\",\"Lata_Mangeshkar_Songs\",\"Muhammad_Rafi_Songs\",\"Asha_Bhosle_Songs\"]\n",
    "  for query,csv_path in zip(Search_Queries,CSV_Paths) :\n",
    "    videos, csv_path = search_youtube_videos(\n",
    "        query,\n",
    "        csv_path,\n",
    "        max_results=200,        # Get top 200 results\n",
    "        max_duration=1200,      # Maximum duration of videos in seconds\n",
    "        min_views=100000,       # Minimum views\n",
    "        min_subs=100000,        # Minimum subscribers\n",
    "        exclude_keywords=[\"remix\", \"cover\", \"mashup\", \"tribute\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Audio from URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract URL from CSV Files and Save into Text Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted links saved to: Data-Scrapping/Extracted_links/Asha_Bhosle_Songs.txt\n",
      "Extracted links saved to: Data-Scrapping/Extracted_links/Lata_Mangeshkar_Songs.txt\n",
      "Extracted links saved to: Data-Scrapping/Extracted_links/mehdi_hasan_songs.txt\n",
      "Extracted links saved to: Data-Scrapping/Extracted_links/Muhammad_Rafi_Songs.txt\n",
      "Extracted links saved to: Data-Scrapping/Extracted_links/Nayyara_Noor_Songs.txt\n",
      "Extracted links saved to: Data-Scrapping/Extracted_links/Tina_Sani_Songs.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing the CSV files\n",
    "folder_path = \"Data-Scrapping/Dataset_Metadata/\"\n",
    "target_path = \"Data-Scrapping/Extracted_links/\"\n",
    "# Ensure the output folder exists\n",
    "os.makedirs(target_path, exist_ok=True)\n",
    "\n",
    "# Process all CSV files in the folder\n",
    "for file in os.listdir(folder_path):\n",
    "    if file.endswith(\".csv\"):\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        output_file_path = os.path.join(target_path, file.replace(\".csv\", \".txt\"))\n",
    "        # os.makedirs(output_file_path, exist_ok=True)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            if 'URL' in df.columns:\n",
    "                df['URL'].dropna().to_csv(output_file_path, index=False, header=False)\n",
    "                print(f\"Extracted links saved to: {output_file_path}\")\n",
    "            else:\n",
    "                print(f\"'links' column not found in {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Scrape Audio from Links and Save in Respective Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"Mehdi Hassan Songs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -f bestaudio --extract-audio --audio-format mp3 --audio-quality 0 \\\n",
    "-o \"Mehdi Hassan Songs/%(title)s.%(ext)s\" \\\n",
    "-a \"Data-Scrapping/extracted_links/mehdi_hasan_songs.txt\" \\\n",
    "--sleep-interval 5 --max-sleep-interval 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"Asha Bhosle Songs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -f bestaudio --extract-audio --audio-format mp3 --audio-quality 0 \\\n",
    "-o \"/kaggle/working/Asha Bhosle Songs/%(title)s.%(ext)s\" \\\n",
    "-a \"/kaggle/input/songs-links/Asha_Bhosle_Songs.txt\" \\\n",
    "--sleep-interval 5 --max-sleep-interval 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"Lata Mangeshkar Songs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -f bestaudio --extract-audio --audio-format mp3 --audio-quality 0 \\\n",
    "-o \"/kaggle/working/Lata Mangeshkar Songs/%(title)s.%(ext)s\" \\\n",
    "-a \"/kaggle/input/songs-links/Lata_Mangeshkar_Songs.txt\" \\\n",
    "--sleep-interval 5 --max-sleep-interval 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"Muhammad Rafi Songs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -f bestaudio --extract-audio --audio-format mp3 --audio-quality 0 \\\n",
    "-o \"/kaggle/working/Muhammad Rafi Songs/%(title)s.%(ext)s\" \\\n",
    "-a \"/kaggle/input/songs-links/Muhammad_Rafi_Songs.txt\" \\\n",
    "--sleep-interval 5 --max-sleep-interval 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"Nayyar Noor Songs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -f bestaudio --extract-audio --audio-format mp3 --audio-quality 0 \\\n",
    "-o \"/kaggle/working/Muhammad Rafi Songs/%(title)s.%(ext)s\" \\\n",
    "-a \"/kaggle/input/songs-links/Nayyara_Noor_Songs.txt\" \\\n",
    "--sleep-interval 5 --max-sleep-interval 15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir \"Tina Sani Songs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yt-dlp -f bestaudio --extract-audio --audio-format mp3 --audio-quality 0 \\\n",
    "-o \"/kaggle/working/Tina Sani Songs/%(title)s.%(ext)s\" \\\n",
    "-a \"/kaggle/input/songs-links/Tina_Sani_Songs.txt\" \\\n",
    "--sleep-interval 5 --max-sleep-interval 15\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
